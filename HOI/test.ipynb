{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11bf36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"INSTALLING REQUIRED PACKAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Install all required packages\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q transformers accelerate\n",
    "!pip install -q ultralytics\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q pandas numpy\n",
    "!pip install -q pytorchvideo\n",
    "!pip install -q decord\n",
    "!pip install -q pillow\n",
    "!pip install -q av\n",
    "!pip install -q huggingface-hub\n",
    "\n",
    "print(\"\\n✓ All packages installed successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70fd46d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "from google.colab import files\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"IMPORTING LIBRARIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\n✓ Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cf5b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DOWNLOADING SAMPLE VIDEOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"data/videos\", exist_ok=True)\n",
    "os.makedirs(\"data/annotations\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# For this demo, we'll use sample videos from EPIC-KITCHENS or create test videos\n",
    "# You can replace these with actual EPIC-KITCHENS videos\n",
    "\n",
    "print(\"\\nNote: For full EPIC-KITCHENS dataset, visit: https://epic-kitchens.github.io/\")\n",
    "print(\"For this demo, we'll work with sample/test videos.\")\n",
    "print(\"\\nYou can upload your own videos using the code below:\")\n",
    "\n",
    "# Uncomment to upload your own videos\n",
    "# uploaded = files.upload()\n",
    "# for filename in uploaded.keys():\n",
    "#     !mv {filename} data/videos/\n",
    "\n",
    "# Create a sample annotation file structure\n",
    "sample_annotations = {\n",
    "    \"video_id\": [],\n",
    "    \"start_frame\": [],\n",
    "    \"end_frame\": [],\n",
    "    \"verb\": [],\n",
    "    \"noun\": [],\n",
    "    \"action\": []\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc9a86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 1: YOLOv8 + VideoMAE Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "class YOLOVideoMAEPipeline:\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        print(\"\\nLoading YOLOv8 model...\")\n",
    "        self.yolo_model = YOLO('yolov8m.pt')  # Medium model for balance\n",
    "        \n",
    "        print(\"Loading VideoMAE model...\")\n",
    "        self.videomae_processor = VideoMAEImageProcessor.from_pretrained(\n",
    "            \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
    "        )\n",
    "        self.videomae_model = VideoMAEForVideoClassification.from_pretrained(\n",
    "            \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
    "        ).to(device)\n",
    "        print(\"✓ Models loaded successfully\")\n",
    "    \n",
    "    def process_video(self, video_path: str, sample_frames: int = 16) -> Dict:\n",
    "        \"\"\"Process video with YOLOv8 for objects and VideoMAE for actions\"\"\"\n",
    "        results = {\n",
    "            \"video_path\": video_path,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"objects_detected\": [],\n",
    "            \"actions_detected\": [],\n",
    "            \"structured_output\": {}\n",
    "        }\n",
    "        \n",
    "        # Read video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        print(f\"\\nProcessing: {video_path}\")\n",
    "        print(f\"Total frames: {total_frames}, FPS: {fps}\")\n",
    "        \n",
    "        # Sample frames uniformly\n",
    "        frame_indices = np.linspace(0, total_frames - 1, sample_frames, dtype=int)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) == 0:\n",
    "            print(\"⚠ No frames extracted\")\n",
    "            return results\n",
    "        \n",
    "        # YOLO Object Detection on middle frame\n",
    "        middle_frame = frames[len(frames) // 2]\n",
    "        yolo_results = self.yolo_model(middle_frame, verbose=False)\n",
    "        \n",
    "        objects = []\n",
    "        for result in yolo_results:\n",
    "            for box in result.boxes:\n",
    "                obj = {\n",
    "                    \"class\": result.names[int(box.cls)],\n",
    "                    \"confidence\": float(box.conf),\n",
    "                    \"bbox\": box.xyxy[0].cpu().numpy().tolist()\n",
    "                }\n",
    "                objects.append(obj)\n",
    "        \n",
    "        results[\"objects_detected\"] = objects\n",
    "        print(f\"✓ Detected {len(objects)} objects\")\n",
    "        \n",
    "        # VideoMAE Action Recognition\n",
    "        # Convert frames to RGB and resize\n",
    "        video_frames = [cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in frames]\n",
    "        video_frames = [Image.fromarray(f) for f in video_frames]\n",
    "        \n",
    "        inputs = self.videomae_processor(video_frames, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.videomae_model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_class = logits.argmax(-1).item()\n",
    "        \n",
    "        action = self.videomae_model.config.id2label[predicted_class]\n",
    "        confidence = torch.softmax(logits, dim=-1).max().item()\n",
    "        \n",
    "        results[\"actions_detected\"] = [{\n",
    "            \"action\": action,\n",
    "            \"confidence\": confidence\n",
    "        }]\n",
    "        \n",
    "        # Structured output for downstream modules\n",
    "        results[\"structured_output\"] = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"duration_seconds\": total_frames / fps if fps > 0 else 0,\n",
    "            \"primary_action\": action,\n",
    "            \"action_confidence\": confidence,\n",
    "            \"objects_in_scene\": [obj[\"class\"] for obj in objects],\n",
    "            \"high_confidence_objects\": [\n",
    "                obj[\"class\"] for obj in objects if obj[\"confidence\"] > 0.5\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ Action detected: {action} (confidence: {confidence:.3f})\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize Model 1\n",
    "print(\"\\nInitializing YOLOv8 + VideoMAE Pipeline...\")\n",
    "model1 = YOLOVideoMAEPipeline(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1716ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: MODEL 2 - LLAVA-1.5\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 2: LLaVA-1.5 (End-to-End VLM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "class LLaVAPipeline:\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        print(\"\\nLoading LLaVA-1.5 model (this may take a few minutes)...\")\n",
    "        \n",
    "        # Using LLaVA-NeXT which is the latest version\n",
    "        model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "        \n",
    "        self.processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "        self.model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        print(\"✓ LLaVA model loaded successfully\")\n",
    "    \n",
    "    def process_video(self, video_path: str, sample_frames: int = 8) -> Dict:\n",
    "        \"\"\"Process video with LLaVA for end-to-end understanding\"\"\"\n",
    "        results = {\n",
    "            \"video_path\": video_path,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"llava_analysis\": {}\n",
    "        }\n",
    "        \n",
    "        # Read video and sample frames\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        frame_indices = np.linspace(0, total_frames - 1, sample_frames, dtype=int)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(Image.fromarray(frame_rgb))\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) == 0:\n",
    "            return results\n",
    "        \n",
    "        # Analyze multiple frames with different prompts\n",
    "        prompts = [\n",
    "            \"What objects can you see in this image? List them.\",\n",
    "            \"What action is being performed in this image?\",\n",
    "            \"Describe what is happening in this scene in detail.\",\n",
    "            \"Is there any safety concern in this image? What objects need attention?\"\n",
    "        ]\n",
    "        \n",
    "        analyses = {}\n",
    "        \n",
    "        for i, prompt in enumerate(prompts):\n",
    "            # Use middle frame for analysis\n",
    "            middle_frame = frames[len(frames) // 2]\n",
    "            \n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            prompt_text = self.processor.apply_chat_template(\n",
    "                conversation, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                images=middle_frame,\n",
    "                text=prompt_text,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            \n",
    "            response = self.processor.decode(\n",
    "                output[0], skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Extract just the answer (after \"ASSISTANT:\")\n",
    "            if \"ASSISTANT:\" in response:\n",
    "                response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "            \n",
    "            analyses[f\"prompt_{i+1}\"] = {\n",
    "                \"question\": prompt,\n",
    "                \"answer\": response\n",
    "            }\n",
    "            \n",
    "            print(f\"✓ Analyzed with prompt {i+1}/{len(prompts)}\")\n",
    "        \n",
    "        results[\"llava_analysis\"] = analyses\n",
    "        \n",
    "        # Create structured output\n",
    "        results[\"structured_output\"] = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"duration_seconds\": total_frames / fps if fps > 0 else 0,\n",
    "            \"objects_mentioned\": analyses.get(\"prompt_1\", {}).get(\"answer\", \"\"),\n",
    "            \"action_description\": analyses.get(\"prompt_2\", {}).get(\"answer\", \"\"),\n",
    "            \"scene_description\": analyses.get(\"prompt_3\", {}).get(\"answer\", \"\"),\n",
    "            \"safety_assessment\": analyses.get(\"prompt_4\", {}).get(\"answer\", \"\")\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize Model 2\n",
    "print(\"\\nInitializing LLaVA Pipeline...\")\n",
    "model2 = LLaVAPipeline(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23701a8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 3: Qwen2-VL (State-of-Art VLM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "class Qwen2VLPipeline:\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        print(\"\\nLoading Qwen2-VL model...\")\n",
    "        \n",
    "        model_id = \"Qwen/Qwen2-VL-2B-Instruct\"  # Using 2B for Colab compatibility\n",
    "        \n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "        print(\"✓ Qwen2-VL model loaded successfully\")\n",
    "    \n",
    "    def process_video(self, video_path: str, sample_frames: int = 8) -> Dict:\n",
    "        \"\"\"Process video with Qwen2-VL\"\"\"\n",
    "        results = {\n",
    "            \"video_path\": video_path,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"qwen_analysis\": {}\n",
    "        }\n",
    "        \n",
    "        # Read video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        frame_indices = np.linspace(0, total_frames - 1, sample_frames, dtype=int)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(Image.fromarray(frame_rgb))\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) == 0:\n",
    "            return results\n",
    "        \n",
    "        # Qwen2-VL prompts for dementia care\n",
    "        prompts = [\n",
    "            \"List all objects visible in this image.\",\n",
    "            \"What hand-object interaction is happening? Describe the action.\",\n",
    "            \"Describe the scene and what activity is being performed.\",\n",
    "            \"From a safety perspective for an elderly person, what should we monitor here?\"\n",
    "        ]\n",
    "        \n",
    "        analyses = {}\n",
    "        \n",
    "        for i, prompt in enumerate(prompts):\n",
    "            middle_frame = frames[len(frames) // 2]\n",
    "            \n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": middle_frame},\n",
    "                        {\"type\": \"text\", \"text\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            inputs = self.processor(\n",
    "                text=[text],\n",
    "                images=[middle_frame],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=128\n",
    "                )\n",
    "            \n",
    "            response = self.processor.batch_decode(\n",
    "                output_ids,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "            \n",
    "            # Extract answer after the prompt\n",
    "            if prompt in response:\n",
    "                response = response.split(prompt)[-1].strip()\n",
    "            \n",
    "            analyses[f\"prompt_{i+1}\"] = {\n",
    "                \"question\": prompt,\n",
    "                \"answer\": response\n",
    "            }\n",
    "            \n",
    "            print(f\"✓ Analyzed with prompt {i+1}/{len(prompts)}\")\n",
    "        \n",
    "        results[\"qwen_analysis\"] = analyses\n",
    "        \n",
    "        # Structured output\n",
    "        results[\"structured_output\"] = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"duration_seconds\": total_frames / fps if fps > 0 else 0,\n",
    "            \"objects_detected\": analyses.get(\"prompt_1\", {}).get(\"answer\", \"\"),\n",
    "            \"action_detected\": analyses.get(\"prompt_2\", {}).get(\"answer\", \"\"),\n",
    "            \"scene_context\": analyses.get(\"prompt_3\", {}).get(\"answer\", \"\"),\n",
    "            \"safety_monitoring\": analyses.get(\"prompt_4\", {}).get(\"answer\", \"\")\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize Model 3\n",
    "print(\"\\nInitializing Qwen2-VL Pipeline...\")\n",
    "model3 = Qwen2VLPipeline(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8c6fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#  ============================================================================\n",
    "# SECTION 7: BENCHMARKING FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BENCHMARKING FRAMEWORK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    def __init__(self, models: Dict):\n",
    "        self.models = models\n",
    "        self.results = []\n",
    "    \n",
    "    def run_benchmark(self, video_path: str) -> Dict:\n",
    "        \"\"\"Run all models on a video and collect results\"\"\"\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"BENCHMARKING VIDEO: {video_path}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        benchmark_results = {\n",
    "            \"video\": video_path,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_results\": {}\n",
    "        }\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"\\n--- Running {model_name} ---\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                result = model.process_video(video_path)\n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                result[\"inference_time\"] = elapsed_time\n",
    "                benchmark_results[\"model_results\"][model_name] = result\n",
    "                \n",
    "                print(f\"✓ {model_name} completed in {elapsed_time:.2f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ {model_name} failed: {str(e)}\")\n",
    "                benchmark_results[\"model_results\"][model_name] = {\n",
    "                    \"error\": str(e),\n",
    "                    \"inference_time\": time.time() - start_time\n",
    "                }\n",
    "        \n",
    "        self.results.append(benchmark_results)\n",
    "        return benchmark_results\n",
    "    \n",
    "    def save_results(self, output_path: str = \"results/benchmark_results.json\"):\n",
    "        \"\"\"Save all benchmark results to JSON\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(f\"\\n✓ Results saved to {output_path}\")\n",
    "    \n",
    "    def generate_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate a summary report\"\"\"\n",
    "        report_data = []\n",
    "        \n",
    "        for result in self.results:\n",
    "            video = result[\"video\"]\n",
    "            for model_name, model_result in result[\"model_results\"].items():\n",
    "                row = {\n",
    "                    \"video\": video,\n",
    "                    \"model\": model_name,\n",
    "                    \"inference_time\": model_result.get(\"inference_time\", None),\n",
    "                    \"success\": \"error\" not in model_result\n",
    "                }\n",
    "                report_data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(report_data)\n",
    "        return df\n",
    "\n",
    "# Initialize benchmark runner\n",
    "models_dict = {\n",
    "    \"YOLOv8_VideoMAE\": model1,\n",
    "    \"LLaVA-1.5\": model2,\n",
    "    \"Qwen2-VL\": model3\n",
    "}\n",
    "\n",
    "benchmark = BenchmarkRunner(models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12971e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: RUN BENCHMARK ON UCF101\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RUNNING BENCHMARK ON UCF101 VIDEOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run benchmark on all selected videos\n",
    "for i, video_path in enumerate(test_videos, 1):\n",
    "    print(f\"\\n[{i}/{len(test_videos)}] Processing: {Path(video_path).parent.name}/{Path(video_path).name}\")\n",
    "    try:\n",
    "        results = benchmark.run_benchmark(video_path)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {video_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save results\n",
    "benchmark.save_results()\n",
    "\n",
    "# Generate report\n",
    "report = benchmark.generate_report()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BENCHMARK REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(report)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = report.groupby('model').agg({\n",
    "    'inference_time': ['mean', 'std', 'min', 'max'],\n",
    "    'success': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save report to CSV\n",
    "report.to_csv('results/benchmark_report.csv', index=False)\n",
    "print(\"\\n✓ Detailed report saved to results/benchmark_report.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
